{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I developed this notebook as a walk-through a relatively basic convolutional neural network for producing bounding boxes for a shape in an image.  For this problem I have an accompanying notebook (trainImgGen) that produces a set of images to train and test our network with.  Remember that the purpose of this neural network is to \"look\" at an image and produce four outputs: the (x,y) coordinate for the bounding boxes top left corner as well as the width and height of the bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#These statements import the different layers, utilities, etc that we need from Keras.\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, Flatten, Dense, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "#These libraries are used for plotting our predictions at the end of this notebook.\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#Numpy is a very popular library for many reason but we'll use its arrays in this notebook.  \n",
    "#You'll notice that numpy as refered to as \"np\" for the rest of the notebook.\n",
    "import numpy as np\n",
    "\n",
    "#A couple other libraries that we'll need\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This block of code is mostly here to allow the GPU (the hardware that speeds up the neural network training)\n",
    "#to be shared between computers.  This is all backend stuff that you don't necessary need to worry about.\n",
    "import tensorflow as tf\n",
    "from keras import backend as k\n",
    " \n",
    "config = tf.ConfigProto()\n",
    " \n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    " \n",
    "# Only allow a total of half the GPU memory to be allocated\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    " \n",
    "# Create a session with the above options specified.\n",
    "k.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below is where we will import all of the training images that were created in the trainImgGen notebook. An important idea that you need to understand is that images can be loaded in an array structure where each element (number in the array) is a \"color\" value of the corresponding pixel.  A simple black-and-white 12 pixel by 12 pixel image could be represented by a 12x12 matrix (called an array in python) where each element is a 1 or 0 (black or white).  \n",
    "\n",
    "If we look at a 12x12 pixel RGB (red-blue-green) image we would have an array that's not just 12x12 but 12x12x3.  The three comes from having a seperate matrix for the red channel, the green channel, and the blue channel.  For each of these color channels you will see numbers between 0 and 255.  \n",
    "\n",
    "The images created by the trainImgGen notebook are RGB images so, even though they are black and white, when we import them here they will come in as arrays of size 12x12x3.  You will also see that we are importing the coordinates of the top corner, width, and height of the black box in each image so we have a \"labeled\" dataset to train our network with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a blank list to store the image arrays.\n",
    "imgList = []\n",
    "#Iterate through all .png files in this folder.\n",
    "for file in os.listdir(\"trainImages\"):\n",
    "    if file.endswith(\".png\"):\n",
    "        filename = 'trainImages/' + file\n",
    "        #import the image as an array\n",
    "        imgArr = np.array(Image.open(filename))\n",
    "        #the new few lines are all for pulling out the box coordinates, width, and height\n",
    "        #find the '-' in the filename so we can parse out what we need\n",
    "        locString = file[file.index('-')+1:file.index('-')+5]\n",
    "        topCornerX = int(locString[0])\n",
    "        topCornerY = int(locString[1])\n",
    "        boxWidth = int(locString[2])\n",
    "        boxHeight = int(locString[3])\n",
    "        #append this image's array, top corner coordinates, width and height to our imgList list\n",
    "        imgList.append((imgArr, topCornerX, topCornerY, boxWidth, boxHeight))\n",
    "\n",
    "#we pull out our image width, height, and number of channels (3 for our images) to use these measurements later\n",
    "#this line just pulls out the first image's array and gets the shape of it\n",
    "imgWidth = imgList[0][0].shape[1]\n",
    "imgHeight = imgList[0][0].shape[0]\n",
    "imgChannel = imgList[0][0].shape[2]\n",
    "#this gives us the total number of images we imported\n",
    "numImg = len(imgList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [  0,   0,   0],\n",
      "        [  0,   0,   0]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [  0,   0,   0],\n",
      "        [  0,   0,   0]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]]], dtype=uint8), 1, 0, 8, 2)\n"
     ]
    }
   ],
   "source": [
    "#I've included this to show you an example of one of the image arrays and \n",
    "#how the top corner coordinates, width and height are stored in the list.\n",
    "print(imgList[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few blocks are to set up our train/test split.  We want to train the network on a set of labeled examples and then \"test\" it on some that it didn't see during training.  One important fact here is that python is \"zero indexed\" meaning the first element in a list, array, etc. is at 0 (not 1 like some other languages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is set up for a 80/20 train/test split.  This first line just gets the number of lines we'll split off\n",
    "#for the \"train\" set of data.\n",
    "splitRow = int(0.80 * len(imgList))\n",
    "#We want this to be a random split so we shuffle the \"rows\" of the list.\n",
    "random.shuffle(imgList)\n",
    "#The training data is from row 0 to splitRow and the testing is from splitRow to the end.\n",
    "train_data = imgList[:splitRow]\n",
    "test_data = imgList[splitRow:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this block I'm going to take the train_data list and form into in a numpy array because that is\n",
    "#the form Keras expects to see our data in.\n",
    "trainSize = len(train_data)\n",
    "\n",
    "#This says that the \"x\" or predictor (the array of each image) for our training is the first element of each row in our list.\n",
    "train_X = [data[0] for data in train_data]\n",
    "\n",
    "#This says that the \"y\" or response (the corner coords, width, and height) are elements one through five of each row in our list.\n",
    "train_Y = [data[1:5] for data in train_data]\n",
    "\n",
    "#This looks confusing but this is where we take our list and make it into an numpy array.  We're doing something called\n",
    "#\"list comprehension\" here because we need to pull out the actual array in each row of the train_X list.  Each array (image)\n",
    "#is then \"vertically\" stacked in our new array and then the array is reshaped into the four dimensions that keras needs it in\n",
    "#for our convolutional layer.\n",
    "train_XArr = np.array([np.vstack(X) for X in train_X]).reshape(trainSize, imgHeight, imgWidth, imgChannel)\n",
    "\n",
    "#A similar thing is happening here with the reponse values as we shape them into the array we needs for our training.\n",
    "train_YInt = np.array([np.vstack(Y) for Y in train_Y]).reshape(trainSize, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next step is very important but may not be immediately obvious but is vital to producing a good neural network.  Neural networks are much better at dealing providing predictions of whether or not something belongs to a certain class instead of providing a definite yes or no. You can absolutely force them into making predictions of ones or zeros but it can significantly hamper your training so, in a problem like we have here, we generally set up our output layer to be a percentage prediction that our output belongs to a set of categories. \n",
    "\n",
    "If our images are 12x12 then we will have an output layers that are 12 nodes wide where each node will be a percentage that the prediction belongs to that category.  For instance, if our bounding box coordinates for a particular image are (2,3) then we'd hope to see one of the outputs of our network (the one for the x-coordinate) be something like \n",
    "```text\n",
    "[0.001, 0.001, 0.989, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
    "```\n",
    "Here you can see that all of the categories add up to one (more on that later) and that the highest prediction is in the third place which would correspond to the second pixel ([pixel 0, pixel 1, pixel 2,...]) or a prediction that the x-coordinate for the top corner of our bounding box is 2.\n",
    "\n",
    "Hopefully this helps explain why we have the code in the block below which takes our four numbers (x,y,width,height) and makes them into what is called a \"one-hot\" vector (where every element except one is a zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_YArr = to_categorical(train_YInt, num_classes=max(imgWidth, imgHeight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X-coordinate: 7\n",
      "One-hot vector: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#This cell is here to demonstrate an example of the one-hot encoding.\n",
    "print(\"X-coordinate:\",train_YInt[0,0])\n",
    "print(\"One-hot vector:\",train_YArr[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This block is an exact copy of what we did with the train_data \n",
    "testSize = len(test_data)\n",
    "test_X = [data[0] for data in test_data]\n",
    "test_Y = [data[1:5] for data in test_data]\n",
    "test_XArr = np.array([np.vstack(X) for X in test_X]).reshape(testSize, imgHeight, imgWidth, imgChannel)\n",
    "test_YInt = np.array([np.vstack(Y) for Y in test_Y]).reshape(testSize, 4)\n",
    "test_YArr = to_categorical(test_YInt, num_classes=max(imgWidth, imgHeight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I stole this from someone else but it just uses Keras' callbacks to produce a graph of the losses during training.\n",
    "#You'll see this graph below.\n",
    "class PlotLosses(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        \n",
    "        self.fig = plt.figure()\n",
    "        \n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.i += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        plt.plot(self.x, self.losses, label=\"loss\")\n",
    "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "        plt.legend()\n",
    "        plt.show();\n",
    "        \n",
    "plot_losses = PlotLosses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we're setting the input shape of the network as (imgWidth, imgHeight, imgChannels).\n",
    "input_shape = tuple(list(train_XArr.shape)[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two model building APIs in Keras: sequential and functional.  Most introductory projects you'll find\n",
    "on the internet are going to use the sequential because it's easier to understand (you add layers one-by-one to your network).  Because we require four outputs from our network, we needed to use the more advanced functional API.  I just want to warn you in case you see something different when looking around on the internet. \n",
    "\n",
    "In the cell below you will see that the layers are added in this general form:\n",
    "LayerName = TypeOfLayer(attributes)(LayerFeedingThisLayer)\n",
    "\n",
    "For this introduction I've built about the most basic network I could that would perform reasonably well. It consists of an input layer, a single convolutional layer, a flattening layer (for reshaping the output of the convolutional layer), and finally four output layers so we can get the four predictions we need.\n",
    "\n",
    "You'll notice that in a lot of this code I reference different parts of the \"shape\" method of various arrays.  It's good practice to do this instead of hard-coding numbers because it provides some flexibility should you want to change image size or other aspects of your inputs or outputs. \n",
    "\n",
    "The Conv2D layer below is the most important of this small network and is, obviously, a convolutional layer. The first attribute in this network is the number of filters to apply to our image.  These filters are relatively small matrices who elements are randomly generated.  These filters are also called kernels and the next attribute \"kernel_size\" gives the size of each of these kernels or filters. I don't want to recreate a complete intro to convolutional neural networks here so hopefully you can find some more intro material elsewhere.  I mentioned these attributes here to encourage you to change the number, recompile, and retrain the network to see the comparative success of smaller networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0717 12:37:27.835039 140639173166912 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0717 12:37:27.835831 140639173166912 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0717 12:37:27.840238 140639173166912 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=input_shape)\n",
    "conv1 = Conv2D(12, kernel_size=(3, 3), strides=(1, 1),\n",
    "                 activation='sigmoid',\n",
    "                 input_shape=input_shape,\n",
    "                    data_format='channels_last')(inputs)\n",
    "flat = Flatten()(conv1)\n",
    "\n",
    "#You'll the activation function for each of the output layer is \"softmax\". This activation forces all of the \n",
    "#values in each node across each layer to add up to one... which is handy if you are looking for percentages.\n",
    "topX = Dense(train_YArr.shape[2], activation='softmax')(flat)\n",
    "topY = Dense(train_YArr.shape[2], activation='softmax')(flat)\n",
    "width = Dense(train_YArr.shape[2], activation='softmax')(flat)\n",
    "height = Dense(train_YArr.shape[2], activation='softmax')(flat)\n",
    "\n",
    "#This last line actually builds the model by identifying the input layer and output layers.\n",
    "model = Model(outputs = [topX, topY, width, height], inputs = inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0717 12:37:27.911499 140639173166912 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0717 12:37:27.919362 140639173166912 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 12, 12, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 10, 10, 12)   336         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1200)         0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 12)           14412       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 12)           14412       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 12)           14412       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 12)           14412       flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 57,984\n",
      "Trainable params: 57,984\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#The compile method sets the loss function as well as optimizer.  Because we are making a categorical prediction we want\n",
    "#to use one of the keras' categorical loss function. Understanding these things any further is beyond the scope of this\n",
    "#introduction.\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(lr=0.001))\n",
    "\n",
    "#This summary is useful in establishing exactly how the inputs and outputs of various model layers are shaped as well\n",
    "#as how many trainable parameters (parameters that will be modified during training) are in the model. \n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"fit\" method is where we actuall train the model.  You can see that the first attribute is our training predictors\n",
    "(the images in the train dataset) and the second attribute is a list of the four responses (x,y,width,height).  When we are doing supervised training we provide the network with predictors and responses so it can calculate the difference between what it predicted and the actual value.  It will use this difference to improve future predictions.  I also provide it with the the test dataset as \"validation_data\" so it can give us some indication of how it is performing on data it hasn't trained on.  \n",
    "\n",
    "There are a couple more important attributes here including \"epochs\" and \"batch_size\" which will require some external research.  Briefly, the batch_size represents how many samples (images) we push through the network during one training cycle.  Remember training neural networks is really a big matrix algebra problem so we just make the matrices bigger when we have bigger batch_sizes.  Epochs are the number of times that each sample in our training dataset will be seen by the network.\n",
    "\n",
    "Once you start training the graph I mentioned below will appear and you'll be able to track the losses as the network improves its ability to make prediction.  We want to see a relatively smooth decreasing function for both the loss and val_loss here.  Occasionally we will see a jump of the loss as the algorithm makes a \"mistake\" in optimizing the network's parameters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe86c5d7c50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_XArr, [train_YArr[:,0],train_YArr[:,1],train_YArr[:,2],train_YArr[:,3]],\n",
    "          validation_data = (test_XArr, [test_YArr[:,0],test_YArr[:,1],test_YArr[:,2],test_YArr[:,3]]),\n",
    "          epochs=100, verbose=0, batch_size=500, callbacks=[plot_losses])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last bit of code here is just to provide you with some indication of what we've been doing above.  It will pull a random image from the test dataset, reshape it to push into the network, and finally use the network to make a prediction of the x, y, width, and height of the bounding box for the black shape in the image.\n",
    "\n",
    "Shift-Enter the cell below several times to see different examples of how well the network works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.patches.Rectangle at 0x7fe7ec27b588>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Get a random image number for an image in our test dataset\n",
    "imgNum = np.random.randint(0, test_XArr.shape[0])\n",
    "\n",
    "#Reshape that image array to it'll go into the network\n",
    "testPredImg = test_XArr[imgNum].reshape(1,test_XArr.shape[1],test_XArr.shape[2],test_XArr.shape[3])\n",
    "\n",
    "#Make the prediction\n",
    "prediction = model.predict(testPredImg)\n",
    "\n",
    "#Getting the column number highest percentage (argmax) of each output layer\n",
    "predCoords = np.argmax(prediction, axis=2).flatten()\n",
    "\n",
    "#Make an image from the array we just pushed into the network\n",
    "img = Image.fromarray(test_XArr[imgNum])\n",
    "\n",
    "#Some matplotlib code\n",
    "fig,ax = plt.subplots(1)\n",
    "\n",
    "#Plot our test image that we just pushed into the network\n",
    "plt.imshow(img, origin='upper', extent=[0,imgWidth,imgHeight,0])\n",
    "\n",
    "#Add a red rectangle where the network predicted the bounding box should be\n",
    "ax.add_patch(patches.Rectangle((predCoords[0],predCoords[1]),\n",
    "                                    (predCoords[2]),\n",
    "                                    (predCoords[3]), \n",
    "                                    fill=False, color='red'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final note, you will find several instances where the network doesn't do very well predicting the bounding box (especially when the shape is on the edge of the image).  Some of this due to requiring more training and a bigger training dataset but some of it is just due to the relative simplicity of our neural network.  No doubt any network you build for your projects will need to be substantially more complicated. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
